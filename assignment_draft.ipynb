{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Assignment\n",
    "\n",
    "Developed by Yongkang Liu  \n",
    "December 16, 2019\n",
    "\n",
    "<a name=\"top\"></a>  \n",
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friendly Reminder: If you want to just check the cleaned data, you can skip the data preparation steps and jump to the end of this notebook. [click here](#cleaned) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "\n",
    "* [Transportation Data and Examples](http://transitdatatoolkit.com/lessons/mapping-a-transit-system/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Source: Turntile Data\n",
    "MTA reguarly publishes the Turntile data every week. Each file contains information regarding the counts of entries and exits through each turntile in MTA stations around every 4 hours. Each turntile is distinguished by UNIT, SCP and STATION. Meanwhile, each station is distinguished by the station name, line name, and division.\n",
    "\n",
    "Major operations on the data include 1) get the per period entries and exits for each turntile (using groupby), 2) combine data within a station by the time (here the time will be checked by a larger time slice), 3) There are some reset record for entries and exits, how to handle these records (delete or incorporate?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C/A</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>SCP</th>\n",
       "      <th>STATION</th>\n",
       "      <th>LINENAME</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>DESC</th>\n",
       "      <th>ENTRIES</th>\n",
       "      <th>EXITS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>10/26/2019</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7247322</td>\n",
       "      <td>2455491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>10/26/2019</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7247336</td>\n",
       "      <td>2455499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>10/26/2019</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7247351</td>\n",
       "      <td>2455532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>10/26/2019</td>\n",
       "      <td>12:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7247463</td>\n",
       "      <td>2455623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>10/26/2019</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7247755</td>\n",
       "      <td>2455679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C/A  UNIT       SCP STATION LINENAME DIVISION        DATE      TIME  \\\n",
       "0  A002  R051  02-00-00   59 ST  NQR456W      BMT  10/26/2019  00:00:00   \n",
       "1  A002  R051  02-00-00   59 ST  NQR456W      BMT  10/26/2019  04:00:00   \n",
       "2  A002  R051  02-00-00   59 ST  NQR456W      BMT  10/26/2019  08:00:00   \n",
       "3  A002  R051  02-00-00   59 ST  NQR456W      BMT  10/26/2019  12:00:00   \n",
       "4  A002  R051  02-00-00   59 ST  NQR456W      BMT  10/26/2019  16:00:00   \n",
       "\n",
       "      DESC  ENTRIES  \\\n",
       "0  REGULAR  7247322   \n",
       "1  REGULAR  7247336   \n",
       "2  REGULAR  7247351   \n",
       "3  REGULAR  7247463   \n",
       "4  REGULAR  7247755   \n",
       "\n",
       "   EXITS                                                                 \n",
       "0                                            2455491                     \n",
       "1                                            2455499                     \n",
       "2                                            2455532                     \n",
       "3                                            2455623                     \n",
       "4                                            2455679                     "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save MTA turnstile data into a dataframe\n",
    "# Source: http://web.mta.info/developers/turnstile.html\n",
    "# The file is downloaded and saved in the same folder as the notebook\n",
    "df_tt = pd.read_csv('turnstile_191102.txt', skipinitialspace=True)  # the data in the week of Nov. 02, 2019\n",
    "df_tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tt.columns: Index(['C/A', 'UNIT', 'SCP', 'STATION', 'LINENAME', 'DIVISION', 'DATE', 'TIME',\n",
      "       'DESC', 'ENTRIES',\n",
      "       'EXITS                                                               '],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the column headers\n",
    "print(f'df_tt.columns: {df_tt.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the last column name, a string variable, contains many space characters. We need to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the last column name: (\"EXITS                                                               \")\n",
      "After the change, the new columns are Index(['C/A', 'UNIT', 'SCP', 'STATION', 'LINENAME', 'DIVISION', 'DATE', 'TIME',\n",
      "       'DESC', 'ENTRIES', 'EXITS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'Check the last column name: (\"{df_tt.columns[-1]}\")')\n",
    "\n",
    "# rename the column name\n",
    "df_tt.rename(columns={df_tt.columns[-1]:df_tt.columns[-1].strip(' ')}, inplace=True)\n",
    "print(f'After the change, the new columns are {df_tt.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better identify each station, we design a unique station index. Since a station is uniquely identified by the station name, line name, and division. We use a number string as the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique station names: 377\n",
      "The number of unique line names: 113\n",
      "The number of unique division names: 6\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of unique station names: {len(df_tt.STATION.unique().tolist())}')\n",
    "print(f'The number of unique line names: {len(df_tt.LINENAME.unique().tolist())}')\n",
    "print(f'The number of unique division names: {len(df_tt.DIVISION.unique().tolist())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use three digits to label a station, three digits for a line combination, and one digit for a division. For example, the station \"RIT-ROOSEVELT\" in Line \"R\" of Division \"RIT\" have the encoded indices, \"376\", \"019\", and \"5\", for the station name, line combination, and division, respectively. We label it as \"t5019376\" in the format of \"t-Division-Line-Station\" where \"t\" stands for the turntile data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, obtain the index in each column\n",
    "dict_tt_stations = {k: str(v).zfill(3) for v, k in enumerate(df_tt.STATION.unique().tolist())}\n",
    "dict_tt_lines = {k: str(v).zfill(3) for v, k in enumerate(df_tt.LINENAME.unique().tolist())}\n",
    "dict_tt_divisions = {k: str(v) for v, k in enumerate(df_tt.DIVISION.unique().tolist())}\n",
    "\n",
    "# Then, create a new column in the dataframe and assign the unique index \n",
    "df_tt['STATION_IDX'] = df_tt[['STATION', 'LINENAME', 'DIVISION']].apply(lambda x: 't'+dict_tt_divisions[x.DIVISION]+dict_tt_lines[x.LINENAME]+dict_tt_stations[x.STATION], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: check elements in each column/feature\n",
    "# Explanations to Terminology can be found in http://web.mta.info/developers/resources/nyct/turnstile/ts_Field_Description.txt\n",
    "# df_tt.DESC.unique()\n",
    "# df_tt[df_tt.DESC=='RECOVR AUD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Stations' Geolocation Information\n",
    "\n",
    "The goal is to assign each row of the turntile data with the station's geolocation information, i.e., the latitude and longitude, for the future association with Foursquare venues.\n",
    "\n",
    "One station may have multiple exits in different geolocations. As we don't have the location information for each turntile, we will only consider per station's data by summarizing all turntiles' data. \n",
    "\n",
    "We will use the MTA's station geolocation dataset. Due to the spelling rules in different datasets, we also need manual check and correction on part of data. The mapping information will be saved in a separate csv file for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us look at the station geolocation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station Data\n",
    "\n",
    "Station data includes information of each station in MTA. It mainly provides the geolocation information, i.e., latitude and longitude of each station. The record is mainly distinguished by the station name, line name, division.\n",
    "\n",
    "Major operations within this data include 1) select stations in Manhattan, 2)  \n",
    "\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/New_York_City_Subway_nomenclature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple versions of such data\n",
    "# Version 1\n",
    "#df_geo = pd.read_csv('DOITT_SUBWAY_STATION_01_13SEPT2010.csv')\n",
    "#df_geo.loc[0]['LINE'].split('-')\n",
    "\n",
    "# Version 2\n",
    "# df_station_entrances = pd.read_csv('NYC_Transit_Subway_Entrance_And_Exit_Data.csv')\n",
    "\n",
    "# Version 3\n",
    "# We are going to use the following data published by MTA\n",
    "# http://web.mta.info/developers/data/nyct/subway/Stations.csv \n",
    "# in GTFS format\n",
    "df_stations = pd.read_csv('Stations.csv')\n",
    "\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unrelated columns\n",
    "df_stations.drop([\"Complex ID\", \"GTFS Stop ID\", 'Line', 'Structure', 'North Direction Label', 'South Direction Label'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The station data has the unique identifier, i.e., Station ID, for each record. Therefore, we don't need to render some other ID for this dataframe. Our next step is to link these Station IDs with the station index created in the turntile data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.Borough.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further retain our disussion within the Manhatten island. Therefore, we only keep the stations in the \"Borough M\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations = df_stations[df_stations.Borough=='M']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way to retrieve Manhattan-only data, i.e., deleting all rows that contain station data out of Manhattan\n",
    "```python\n",
    "indexNames = df_stations[ df_stations['Borough'] != 'M' ].index # first, get indices of these rows\n",
    "df_stations.drop(indexNames , inplace=True) # remove them from the dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We found {df_stations.shape[0]} stations with {df_stations[\"Stop Name\"].nunique()} unique station names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we have more station records than the unique station names is because one station can have multiple records if it hosts multiple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stations in different Routes may have the same name. Their coordinates may differ from each other but all in a vicinity. \n",
    "# Since the turntile data uses the stop name for all routes, we will render a coordinate for each hub station \n",
    "\n",
    "# check how many records are there for the station name of \"Canal St\"\n",
    "df_stations[df_stations['Stop Name']=='Canal St']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may result in two consequences: 1) one station in the turntile data may have multiple geolocations after the mapping, 2) there may be some wrong mapping due to stations in different lines/divisions share the same name.\n",
    "\n",
    "To mitigate such errors, we use the combination including division, line, and stop name to uniquely locate a station with the unique Station ID. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Stations\n",
    "\n",
    "In order to find all turntile data of Manhattan stations and assign them with the correct geolocation information, we need to match the records by the station names in the df_stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the Turntile dataset\n",
    "\n",
    "# Since there are only three divisions in Manhattan, we first reduce the turntile data size \n",
    "df_tt[df_tt.DIVISION.isin(['BMT', 'IND', 'IRT'])].STATION.nunique()\n",
    "\n",
    "# Create a unique search id for each station\n",
    "df_tt['SEARCH_ID'] = df_tt[['STATION', 'LINENAME', 'DIVISION', 'STATION_IDX']].apply(lambda x: ','.join(x.dropna().astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all stations in the Turntile dataset and save them into a list\n",
    "tt_list = df_tt['SEARCH_ID'].unique().tolist()\n",
    "tt_list = list(map(lambda x: x.split(','), tt_list))\n",
    "\n",
    "# Convert the list to a dataframe\n",
    "df_tt_list = pd.DataFrame(tt_list, columns=['Station', 'Routes', 'Division', 'STATION_IDX'])\n",
    "df_tt_list.head()\n",
    "\n",
    "# Adjust the name format: change \"a-b\" to \"a - b\"\n",
    "def hyphen_adjust(x):\n",
    "    if '-' in x:\n",
    "        tmp = x.split('-')\n",
    "        return ' - '.join(tmp)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "#df_tt_list['Station'] = df_tt_list['Station'].apply(hyphen_adjust)\n",
    "#df_tt_list['Routes'] = df_tt_list['Routes'].apply(lambda x: set(x))\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "df_tt_list['Station'] = df_tt_list['Station'].progress_apply(hyphen_adjust)\n",
    "df_tt_list['Routes'] = df_tt_list['Routes'].progress_apply(lambda x: set(x))\n",
    "# a progress bar will appear when running the code\n",
    "\n",
    "df_tt_list.head()\n",
    "\n",
    "df_tt_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need also to obtain the search ID for the geolocation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the station profile dataset, i.e., the station geolocation reference\n",
    "# Obtain unique ID for the qualified stations\n",
    "df_stations['Search ID'] = df_stations[['Stop Name', 'Daytime Routes', 'Division', 'Station ID']].apply(lambda x: ','.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "df_stations.reset_index(inplace=True)\n",
    "\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sta_list = df_stations['Search ID'].unique().tolist()\n",
    "\n",
    "sta_list = list(map(lambda x: x.upper(), sta_list))  # Captalize all names\n",
    "\n",
    "sta_list = list(map(lambda x: x.split(','), sta_list))\n",
    "\n",
    "df_station_list = pd.DataFrame(sta_list, columns=['Station', 'Routes', 'Division', 'Station ID'])  # save into a dataframe\n",
    "\n",
    "df_station_list['Routes'] = df_station_list['Routes'].apply(lambda x: set(x.split(' '))) # split routes into a list\n",
    "\n",
    "df_station_list.head()\n",
    "\n",
    "df_station_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the job is to find these 153 subway stations in Manhattan in the station list of the Turntile data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new column to save the matched station\n",
    "df_tt_list['Geo_ID']=df_tt_list['Station'].apply(lambda x: [])\n",
    "\n",
    "# Define a new column to signal the match result in the reference station dataframe and set initial values to \"False\" \n",
    "df_station_list['Matched']=df_station_list['Station'].apply(lambda x: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def station_match(tt, station):\n",
    "    matched = False\n",
    "    #print(f'tt: {tt}, station: {station}')\n",
    "    if tt[2] == station[2]:  # the same division\n",
    "        if tt[0] == station[0]:  # the same name\n",
    "            #print(f'Station: {station} matches with TT: {tt}')\n",
    "            if station[1].issubset(tt[1]):  # route set match\n",
    "                matched = True\n",
    "                print(f'Station: {station} matches with TT: {tt} in lines {station[1]}')\n",
    "    return matched\n",
    "'''\n",
    "# a relaxed version which removes the division comparison because some stations in the Turntile data can be a union of multiple stations of different divisions\n",
    "# The station name plus service routes can adequately define a unique station\n",
    "def station_match(tt, station):\n",
    "    matched = False\n",
    "    #print(f'tt: {tt}, station: {station}')\n",
    "    if tt[0] == station[0]:\n",
    "        #print(f'Station: {station} matches with TT: {tt}')\n",
    "        if station[1].issubset(tt[1]):\n",
    "            matched = True\n",
    "            print(f'Station: {station} matches with TT: {tt} in lines {station[1]}')\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "dTest = {}\n",
    "dict_stations = {}\n",
    "for i in range(df_station_list.shape[0]):\n",
    "    for j in range(df_tt_list.shape[0]):\n",
    "        if station_match(list(df_tt_list.loc[j]), list(df_station_list.loc[i])):\n",
    "            count += 1\n",
    "            # Add the Station_ID into The turntile record \n",
    "            df_tt_list.loc[j]['Geo_ID'].append(df_station_list.at[i, \"Station ID\"])\n",
    "            df_station_list.at[i, 'Matched'] = True\n",
    "            if df_tt_list.loc[j]['STATION_IDX'] in dTest:\n",
    "                dict_stations[df_tt_list.loc[j]['STATION_IDX']].append(df_station_list.at[i, \"Station ID\"])\n",
    "            else:\n",
    "                dict_stations[df_tt_list.loc[j]['STATION_IDX']] = [df_station_list.at[i, \"Station ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_list['Matched'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We matched 112 stations by using the exact name search. Let's examine the unmatched cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_list[~df_station_list['Matched']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to manually find those unrecognized stations in the turntile data. The main reasons of failed matches include the format mismatch, different abbreviation, order of words, etc. Since we only have 41 such items, a manual correction is feasible. That is part of the job for data science projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use one case as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_station_manual = df_station_list[~df_station_list['Matched']]\n",
    "station_index = df_station_manual.index\n",
    "iterId = 0\n",
    "if iterId < len(station_index):\n",
    "    print(f'Working on #{iterId} unmatched record with index: {station_index[iterId]}')\n",
    "    print(f'The station name: {df_station_manual.iloc[iterId].Station}, routes: {df_station_manual.iloc[iterId].Routes}, div: {df_station_manual.iloc[iterId].Division}, Station ID: {df_station_manual.iloc[iterId][\"Station ID\"]}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_station_manual[\"Station ID\"].T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unmatched station one after another and save the result into the manually input dictionary\n",
    "keyword2search = 'LEXINGTON'\n",
    "df_tt_list[df_tt_list['Station'].str.contains(keyword2search)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the turntile name, there is no station with the name of \"LEXINGTON AV/59 ST\". Using the keyword \"LEXINGTON\", the target is not in the returned stations. Then, we turn to try another keyword \"59\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword2search = '59'\n",
    "df_tt_list[df_tt_list['Station'].str.contains(keyword2search)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using another keyword \"59\", I can find a number of stations in which the third items in the list.\n",
    "\n",
    "Here, I need to map the \"STATION_IDX\" value, i.e., \"t0004000\" in the turntile data to the \"Station ID\" value, i.e., \"7\", in the geolocation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved these manually identified pairs in a file, 'manual_map.txt'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To facilitate the following processing, we further process the manually rendered mapping information and save the pairs into a csv file.\n",
    "df_manual_match = pd.read_csv('manual_map.txt', skipinitialspace=True)\n",
    "\n",
    "headers = [\"turntile.station\", \"turntile.routes\", \"turntile.station_id\", \"geo.station\", \"geo.routes\", \"geo.station_id\"]\n",
    "rows2write = []\n",
    "rows2write.append(headers)\n",
    "\n",
    "for i in range(df_manual_match.shape[0]):\n",
    "    geo_id = df_manual_match.at[i, \"station_df_id\"]\n",
    "    index_tt = df_manual_match.at[i, \"tt_df_id\"]\n",
    "    print('\\n')\n",
    "    print(f'Manually input station-geo pair #{i}')\n",
    "    print(f'Turntile Station {df_tt_list.at[index_tt, \"Station\"]} w/ Routes: {df_tt_list.at[index_tt, \"Routes\"]}, Station ID: {df_tt_list.at[index_tt, \"STATION_IDX\"]}')\n",
    "    print(f'Geo: {df_station_list.at[geo_id-1, \"Station\"]} w/ Routes: {df_station_list.at[geo_id-1, \"Routes\"]}')\n",
    "    row = [df_tt_list.at[index_tt, \"Station\"], df_tt_list.at[index_tt, \"Routes\"], str(df_tt_list.at[index_tt, \"STATION_IDX\"]), \\\n",
    "          df_station_list.at[geo_id-1, \"Station\"], df_station_list.at[geo_id-1, \"Routes\"], str(df_station_list.at[geo_id-1, \"Station ID\"])]\n",
    "    rows2write.append(row)\n",
    "\n",
    "import csv\n",
    "wrCSVfilename = 'turntile_station_map.csv'\n",
    "\n",
    "with open(wrCSVfilename, mode='a', newline='') as rtd_file:\n",
    "    csv_writer = csv.writer(rtd_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i in rows2write:\n",
    "        #print(i[cols['frame_number']],i[cols['msgCopy']])\n",
    "        csv_writer.writerow(i)\n",
    "print('Writing Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_map = pd.read_csv('turntile_station_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I add these manually found station mappings into the dictionary which already contains the earlier obtained mapping pairs in the automatic matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_manual_map.shape[0]):\n",
    "    if df_manual_map[\"turntile.station_id\"][i] in dTest:\n",
    "        dict_stations[df_manual_map[\"turntile.station_id\"][i]].append(df_manual_map[\"geo.station_id\"][i])\n",
    "    else:\n",
    "        dict_stations[df_manual_map[\"turntile.station_id\"][i]] = [df_manual_map[\"geo.station_id\"][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_map[\"turntile.station_id\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dict_stations:\n",
    "    print(f'There is a key: {i} w/ value of {dict_stations[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a dictionary with the key:value pair of turntile_station_id:station_id. Next, I will use this dictionary and available information to create another dictionary to map the key of turntil_station_id to the station coordinates. As checked earlier, it is found that some station may be associated with multiple coordinates because it hosts multiple lines of services. In such a case, I will use the centroid of the point set instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_stations\n",
    "# df_stations\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "dict_loc = {}\n",
    "\n",
    "for i in dict_stations:\n",
    "    lat = df_stations[df_stations[\"Station ID\"].isin(dict_stations[i])][\"GTFS Latitude\"].tolist()\n",
    "    lat = mean(lat)\n",
    "    lat = round(lat, 6)\n",
    "    long = df_stations[df_stations[\"Station ID\"].isin(dict_stations[i])][\"GTFS Longitude\"].tolist()\n",
    "    long = mean(long)\n",
    "    long = round(long, 6)\n",
    "    print(f'Station: {i} has the coordinates: ({lat}, {long})')\n",
    "    dict_loc[i] = (lat, long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_m = df_tt[df_tt.STATION_IDX.isin(dict_stations)].copy()  # make a copy of a slice of dataframe df_tt\n",
    "\n",
    "# If df_tt_m is set using the get method below\n",
    "# df_tt_m = df_tt[df_tt.STATION_IDX.isin(dict_stations)]\n",
    "# The following changes on df_tt_m would generate warnings to alert about such change may affect the original dataframe\n",
    "# It is because even pandas does not know df_tt_m is a copy or a view of df_tt\n",
    "\n",
    "df_tt_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Turntile data in Manhattan has {df_tt_m.shape[0]} records compared to the total {df_tt.shape[0]} data records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tt_m.loc[:,\"LOCATION\"] = df_tt_m.apply(lambda x: dict_loc[x[\"STATION_IDX\"]], axis=1)\n",
    "\n",
    "#df_tt_m.loc[:,\"LOCATION\"] = df_tt_m[\"STATION_IDX\"].apply(lambda x: dict_loc[x])\n",
    "\n",
    "df_tt_m[\"LOCATION\"] = df_tt_m[\"STATION_IDX\"].map(lambda x: dict_loc[x])\n",
    "\n",
    "df_tt_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_tt_m.groupby(['UNIT', 'SCP', 'STATION', 'LINENAME', 'DIVISION'])  # divide rows into groups based on selected columns as an index \n",
    "\n",
    "# Use dataframe.diff() to calculate the difference between two consecutive rows regarding a specific column\n",
    "# The first row has \"NaN\" values after calculation\n",
    "df_tt_m['ENTRIES_DIFF']=df_grouped[['ENTRIES']].diff()\n",
    "df_tt_m['EXITS_DIFF']=df_grouped[['EXITS']].diff()\n",
    "\n",
    "df_tt_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_m = df_tt_m[~df_tt_m['ENTRIES_DIFF'].isnull()]  # remove all rows with 'NaN' in the 'ENTRIES_DIFF' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#top)\n",
    "<a name=\"cleaned\"></a>  \n",
    "### Cleaned Data\n",
    "\n",
    "Now, we've obtained a clean dataframe for the MTA turntile data. It contains 1) per turntile entries and exits counts every four hours, 2) per station geolocation coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_m.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the cleaned data into a csv for future analysis work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_m.to_csv(\"modified_turntile_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rush hour in Manhattan is 7 to 9 am and 4:30 pm to 7 pm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different venues have different target clients and their distribution may vary with location and time. In Manhattan, the rush hour is usually defined as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance calculation between two points using their latitude and longitude information\n",
    "# Use geopy module\n",
    "# https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n",
    "\n",
    "import geopy.distance\n",
    "\n",
    "coords_1 = (40.576209, -73.967875)\n",
    "coords_2 = (40.576507, -73.969445)\n",
    "\n",
    "d = geopy.distance.distance(coords_1, coords_2).m\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_tt.groupby(['UNIT', 'SCP', 'STATION', 'LINENAME', 'DIVISION'])\n",
    "df_tt_grouped = df_tt.groupby(['STATION', 'LINENAME', 'DIVISION'])[['STATION', 'LINENAME', 'DIVISION']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
